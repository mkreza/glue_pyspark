{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "238624d5-1eec-431d-846f-42011cc9d006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/01 23:45:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "/home/glue_user/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "23/05/01 23:45:35 WARN JobBookmark$: Disabling Job Bookmarks due to an error faced during bookmark service initialization. Job Bookmarks is also disabled in developer environment. Failure reason: \n",
      "java.lang.ClassNotFoundException: com.amazonaws.services.glue.util.AWSGlueJobBookmarkService\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
      "\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
      "\tat java.lang.Class.forName0(Native Method)\n",
      "\tat java.lang.Class.forName(Class.java:264)\n",
      "\tat com.amazonaws.services.glue.util.JobBookmark$.$anonfun$configure$1(JobBookmarkUtils.scala:64)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat com.amazonaws.services.glue.util.JobBookmark$.configure(JobBookmarkUtils.scala:63)\n",
      "\tat com.amazonaws.services.glue.util.Job$.init(Job.scala:96)\n",
      "\tat com.amazonaws.services.glue.util.Job.init(Job.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col, unix_timestamp, from_unixtime, date_format, concat, concat_ws, lit, first, last, min\n",
    "from pyspark.sql.window import Window\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "import sys\n",
    "sys.path.insert(1, 'src')\n",
    "from pyspark.conf import SparkConf\n",
    "from pathlib import Path\n",
    "\n",
    "# Manually Pass Args\n",
    "sys.argv.append('--JOB_NAME')\n",
    "sys.argv.append('test_iceberg')\n",
    "sys.argv.append('--iceberg_job_catalog_warehouse')\n",
    "sys.argv.append('s3://aws-poc-glue/boto/iceberg/')\n",
    "\n",
    "# Spark Configuration\n",
    "# args = getResolvedOptions(sys.argv, ['JOB_NAME', 'iceberg_job_catalog_warehouse'])\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'iceberg_job_catalog_warehouse'])\n",
    "conf = SparkConf()\n",
    "\n",
    "## Please make sure to pass runtime argument --iceberg_job_catalog_warehouse with value as the S3 path \n",
    "conf.set(\"spark.sql.catalog.job_catalog.warehouse\", args['iceberg_job_catalog_warehouse'])\n",
    "conf.set(\"spark.sql.catalog.job_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "conf.set(\"spark.sql.catalog.job_catalog.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n",
    "conf.set(\"spark.sql.catalog.job_catalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "conf.set(\"spark.sql.iceberg.handle-timestamp-without-timezone\",\"true\")\n",
    "\n",
    "# Init Job\n",
    "sc = SparkContext(conf=conf)\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "sc.setCheckpointDir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d687e74f-482d-493e-a7b0-e44ca448dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load DataFrames from checkpoints if they are available\n",
    "\"\"\"\n",
    "p = Path('checkpoints')\n",
    "checklist = ['df_message', 'df_dag', 'df_dagmem', 'df_inbounds', 'df_compkey', 'df_responses', 'df_joined', 'df_inbounds_matched']\n",
    "checkpoints = [x.name for x in p.iterdir() if (x.is_dir() & (x.name in checklist))]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60b9e8a1-c51e-4cee-86d6-324f8a7170af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fetching all required data into DataFrames\n",
    "\"\"\"\n",
    "if not set(checklist[:3]) <= set(checklist):\n",
    "    # Fetch from Data Catalog\n",
    "    df_message = glueContext.create_data_frame.from_catalog(\n",
    "        database=\"boto3\",\n",
    "        table_name=\"message_iceberg\"\n",
    "    )\n",
    "\n",
    "    df_dag = glueContext.create_data_frame.from_catalog(\n",
    "        database=\"boto3\",\n",
    "        table_name=\"dealer_associate_group_iceberg\"\n",
    "    )\n",
    "\n",
    "    df_dagmem = glueContext.create_data_frame.from_catalog(\n",
    "        database=\"boto3\",\n",
    "        table_name=\"dealer_associate_group_member_iceberg\"\n",
    "    )\n",
    "\n",
    "    # Filter the Dealer DataFrame for Relevant columns\n",
    "    df_message = df_message.filter(col(\"protocol\").isin(\"E\",\"X\") & (col(\"ismanual\")==\"1\") & col(\"messagetype\").isin(\"I\",\"S\"))\\\n",
    "        .select(\"id\",\n",
    "                \"dealerid\",\n",
    "                \"messagetype\",\n",
    "                \"protocol\",\n",
    "                \"customerid\",\n",
    "                \"dealerassociateid\",\n",
    "                \"ismanual\",\n",
    "                col(\"senton\").alias(\"timestamp\"))\\\n",
    "        .dropna(subset=[\"timestamp\"])\n",
    "\n",
    "    # Select only the BDC groups and produce a concise list of dealerassociateids, group name and virtual id\n",
    "    df_dag = df_dag.filter(col(\"name\").isin(\"Service BDC\", \"Service Central\"))\\\n",
    "                   .select(\"id\", \"name\", \"virtualdealerassociateid\")\\\n",
    "                   .dropDuplicates()\n",
    "\n",
    "    # Join on BDC Group Members\n",
    "    df_dagmem = df_dagmem.select(\"dealerassociategroupid\", \"dealerassociateid\")\\\n",
    "                         .join(df_dag, df_dagmem['dealerassociategroupid']==df_dag['id'], 'inner')\\\n",
    "                         .drop(\"id\", \"dealerassociategroupid\")\n",
    "\n",
    "    # Produce a concise list of groups and virtual id names\n",
    "    df_dag = df_dag.select(\"name\", \"virtualdealerassociateid\").dropDuplicates()\n",
    "\n",
    "    # Tag Virtual IDs in the Message Table\n",
    "    df_message = df_message.join(\n",
    "                    df_dag.select(\"virtualdealerassociateid\").withColumn('isvirtual', lit(True)),\n",
    "                    df_message['dealerassociateid']==df_dag['virtualdealerassociateid'],\n",
    "                    'left')\\\n",
    "                    .fillna(False)\n",
    "\n",
    "    # Tag which outbound (reps) belong to BDC (virtual) groups\n",
    "    df_message = df_message.join(\n",
    "                    df_dagmem.select(\"dealerassociateid\").withColumn('isvirtualrep', lit(True)),\n",
    "                    'dealerassociateid',\n",
    "                    'left'\n",
    "                    )\\\n",
    "                    .fillna(False)\n",
    "\n",
    "    # Creating Time Bins\n",
    "    df_message = df_message.withColumn('ut', unix_timestamp(col(\"timestamp\"), 'yyyy-MM-dd HH:mm:ss'))\\\n",
    "                           .withColumn('dty', from_unixtime('ut'))\\\n",
    "                           .withColumn('date', date_format('dty', 'yyyy-MM-dd'))\\\n",
    "                           .withColumn('hour', date_format('dty', 'HH').alias('hour'))\n",
    "\n",
    "    # Produce composite key column\n",
    "    df_message = df_message.withColumn(\"compositekey\", concat_ws('-',df_message.dealerid, df_message.date, df_message.hour))\n",
    "    \n",
    "    # Create Checkpoints\n",
    "    df_message.checkpoint()\n",
    "    df_dag.checkpoint()\n",
    "    df_dagmem.checkpoint()\n",
    "    \n",
    "    # Save the DataFrames\n",
    "    df_message.write.format(\"parquet\").mode(\"overwrite\").save(\"checkpoints/df_message\")\n",
    "    df_dag.write.format(\"parquet\").mode(\"overwrite\").save(\"checkpoints/df_dag\")\n",
    "    df_dagmem.write.format(\"parquet\").mode(\"overwrite\").save(\"checkpoints/df_dagmem\")\n",
    "else:\n",
    "    # Load the DataFrames from the checkpoints\n",
    "    df_message = spark.read.format(\"parquet\").load(\"checkpoints/df_message\")\n",
    "    df_dag = spark.read.format(\"parquet\").load(\"checkpoints/df_dag\")\n",
    "    df_dagmem = spark.read.format(\"parquet\").load(\"checkpoints/df_dagmem\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e554e16-16aa-48ad-b733-68dc8b31e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating Composite Key Table:\n",
    ">>> This will be used as the common table for joins\n",
    ">>> The current logic defines the composite key as the unique values between dealerid, date and hour\n",
    ">>> Note: dealerassociateid is included in this table, but is not a part of the actual composite key. Recall that a customer sending an inbound to the BDC inbox, sends it to the virtualdealerassociateid. Therefore, it would be impossible to directly link a response to an inbound, since the responder's id will not correspond with this virtual id. Therefore, we must omit the dealerassociateid in the composite key definition, and retain it as an entry in this table to calculate who responded to what message from which customer.\n",
    "\"\"\"\n",
    "if not 'df_compkey' in checkpoints:\n",
    "    df_compkey = df_message.select(\"compositekey\", \"dealerid\", \"date\", \"hour\").dropDuplicates()\n",
    "    \n",
    "    # Create checkpoint\n",
    "    df_compkey.checkpoint()\n",
    "    \n",
    "    # Save the DataFrame\n",
    "    df_compkey.write.format(\"parquet\").mode(\"overwrite\").save(\"checkpoints/df_compkey\")\n",
    "else:\n",
    "    df_compkey = spark.read.format(\"parquet\").load(\"checkpoints/df_compkey\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "516ad4c0-6fef-470f-8238-e39b0a1d927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Separating Inbound and Response Logic\n",
    ">>> This groups each valid inbound based on the criteria defined under the compositekey\n",
    "\"\"\"\n",
    "if not set(['df_inbounds', 'df_responses', 'df_joined']) <= set(checkpoints):\n",
    "    # Inbound Aggregations\n",
    "    df_inbounds = df_message.filter((col(\"isvirtual\")==lit(True)) & (col(\"messagetype\")=='I'))\n",
    "    # Response Logic\n",
    "    df_responses = df_message.filter((col(\"isvirtualrep\")==lit(True)) & (col(\"messagetype\")=='S'))\n",
    "\n",
    "    # Join the two dataframes\n",
    "    df_joined = df_inbounds.join(df_responses.select(\n",
    "                                        \"customerid\",\n",
    "                                        \"date\",\n",
    "                                        col(\"dealerassociateid\").alias(\"response_dealerassociateid\"),\n",
    "                                        col(\"ut\").alias(\"response_ut\"),\n",
    "                                        col(\"timestamp\").alias(\"response_timestamp\")),\n",
    "                                     ['customerid', 'date'],\n",
    "                                      \"left\")\n",
    "\n",
    "    # Filter for responses which come after an inbound message\n",
    "    df_joined = df_joined.filter(col(\"response_ut\")>col(\"ut\"))\n",
    "    \n",
    "    # Create checkpoints\n",
    "    df_inbounds.checkpoint()\n",
    "    df_responses.checkpoint()\n",
    "    df_joined.checkpoint()\n",
    "    \n",
    "    # Save the DataFrames\n",
    "    df_inbounds.write.format(\"parquet\").mode(\"overwrite\").save(\"checkpoints/df_inbounds\")\n",
    "    df_responses.write.format(\"parquet\").mode(\"overwrite\").save(\"checkpoints/df_responses\")\n",
    "    df_joined.write.format(\"parquet\").mode(\"overwrite\").save(\"checkpoints/df_joined\")\n",
    "else:\n",
    "    df_inbounds = spark.read.format(\"parquet\").load(\"checkpoints/df_inbounds\")\n",
    "    df_responses = spark.read.format(\"parquet\").load(\"checkpoints/df_responses\")\n",
    "    df_joined = spark.read.format(\"parquet\").load(\"checkpoints/df_joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad5268c-09c5-4e9f-a6bf-4a7e90a252fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inbounds_denorm = df_inbounds.groupBy(\"compositekey\", \"dealerassociateid\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c54eee90-f834-4758-8608-11713baacb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import min\n",
    "# # Create a window specification\n",
    "# # w = Window.partitionBy([col(\"id\")]).orderBy(col(\"response_ut\"))\n",
    "# w = Window.partitionBy([col(\"id\")])\n",
    "\n",
    "# # Add a new column with the first value greater than the current row\n",
    "# result_df = joined_df.withColumn(\"first_response\", min(col(\"response_ut\")).over(w)).dropDuplicates()\n",
    "\n",
    "# result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a29e54b4-d087-4111-997b-612d052d9a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tag each inbound which has been responded to with the response timestamp\n",
    "\"\"\"\n",
    "if not 'df_response_tag' in checkpoints:\n",
    "    df_response_tag = df_joined.groupBy(\"id\").agg(min(\"response_ut\").alias(\"response_ut\"))\n",
    "    \n",
    "    # Create Checkpoint\n",
    "    df_response_tag.checkpoint()\n",
    "    \n",
    "    # Save the DataFrame\n",
    "    df_response_tag.write.format(\"parquet\").mode(\"overwrite\").save(\"checkpoints/df_response_tag\")\n",
    "else:\n",
    "    # Load the DataFrame\n",
    "    df_response_tag = spark.read.format(\"parquet\").load(\"checkpoints/df_response_tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "681cdefe-555c-4186-a5e1-bc86b54f021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|       id|response_ut|\n",
      "+---------+-----------+\n",
      "|669446488| 1681985562|\n",
      "|669212359| 1681974429|\n",
      "|668550344| 1681894565|\n",
      "|667979444| 1681819915|\n",
      "|667906144| 1681816802|\n",
      "|667762728| 1681809340|\n",
      "|669996442| 1682060894|\n",
      "|667879895| 1681814868|\n",
      "|667730921| 1681808887|\n",
      "|667885472| 1681815473|\n",
      "|665697753| 1681460253|\n",
      "|670045590| 1682062589|\n",
      "|668737774| 1681903814|\n",
      "|667898591| 1681815781|\n",
      "|669996974| 1682060894|\n",
      "|668174649| 1681833062|\n",
      "|667585655| 1681800749|\n",
      "|669974506| 1682058803|\n",
      "|667706137| 1681808095|\n",
      "|668771070| 1681905355|\n",
      "+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_response_tag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45952123-5536-4d16-b98a-0e67dfdb2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Join the response tags to the inbound dataset\n",
    "\"\"\"\n",
    "if not 'df_inbounds_matched' in checkpoints:\n",
    "    df_inbounds_matched = df_inbounds.join(response_df, 'id', 'left')\n",
    "    df_inbounds_matched = df_inbounds_matched.withColumn(\"response_timestamp\", from_unixtime(col(\"response_ut\")))\n",
    "    \n",
    "    # Create Checkpoint\n",
    "    df_inbounds_matched.checkpoint()\n",
    "    \n",
    "    # Save the DataFrame\n",
    "    df_inbounds_matched.write.format(\"parquet\").mode(\"overwrite\").save(\"checkpoints/df_inbounds_matched\")\n",
    "else:\n",
    "    # Load the DataFrame\n",
    "    df_inbounds_matched = spark.read.format(\"parquet\").load(\"checkpoints/df_inbounds_matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21fcde4e-a51a-4c9e-b89b-0747de2e9e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+--------+-----------+--------+----------+--------+-------------------+------------------------+---------+------------+----------+-------------------+----------+----+------------------+-----------+-------------------+\n",
      "|       id|dealerassociateid|dealerid|messagetype|protocol|customerid|ismanual|          timestamp|virtualdealerassociateid|isvirtual|isvirtualrep|        ut|                dty|      date|hour|      compositekey|response_ut| response_timestamp|\n",
      "+---------+-----------------+--------+-----------+--------+----------+--------+-------------------+------------------------+---------+------------+----------+-------------------+----------+----+------------------+-----------+-------------------+\n",
      "|670727153|           104895|     455|          I|       E| 146267814|       1|2023-04-22 06:27:58|                  104895|     true|       false|1682144878|2023-04-22 06:27:58|2023-04-22|  06| 455-2023-04-22-06|       null|               null|\n",
      "|670688802|            70218|      79|          I|       E|  17798133|       1|2023-04-22 04:36:51|                   70218|     true|       false|1682138211|2023-04-22 04:36:51|2023-04-22|  04|  79-2023-04-22-04|       null|               null|\n",
      "|670960382|           186011|     481|          I|       X|  78068679|       1|2023-04-22 12:49:32|                  186011|     true|       false|1682167772|2023-04-22 12:49:32|2023-04-22|  12| 481-2023-04-22-12|       null|               null|\n",
      "|670937425|            68635|    1221|          I|       X|  81317905|       1|2023-04-22 12:02:18|                   68635|     true|       false|1682164938|2023-04-22 12:02:18|2023-04-22|  12|1221-2023-04-22-12|       null|               null|\n",
      "|670821191|            55896|    1103|          I|       X| 117376841|       1|2023-04-22 08:58:49|                   55896|     true|       false|1682153929|2023-04-22 08:58:49|2023-04-22|  08|1103-2023-04-22-08| 1682153936|2023-04-22 08:58:56|\n",
      "|671022034|           120525|    1680|          I|       X| 104332240|       1|2023-04-22 16:27:18|                  120525|     true|       false|1682180838|2023-04-22 16:27:18|2023-04-22|  16|1680-2023-04-22-16|       null|               null|\n",
      "|670928767|            55896|    1103|          I|       X|  62629256|       1|2023-04-22 11:46:48|                   55896|     true|       false|1682164008|2023-04-22 11:46:48|2023-04-22|  11|1103-2023-04-22-11| 1682164033|2023-04-22 11:47:13|\n",
      "|670902449|           120525|    1680|          I|       X| 111252060|       1|2023-04-22 11:01:49|                  120525|     true|       false|1682161309|2023-04-22 11:01:49|2023-04-22|  11|1680-2023-04-22-11| 1682162270|2023-04-22 11:17:50|\n",
      "|670953150|            68642|    1230|          I|       X|  95508447|       1|2023-04-22 12:33:38|                   68642|     true|       false|1682166818|2023-04-22 12:33:38|2023-04-22|  12|1230-2023-04-22-12| 1682167189|2023-04-22 12:39:49|\n",
      "|670842244|           120525|    1680|          I|       X|  96994872|       1|2023-04-22 09:29:20|                  120525|     true|       false|1682155760|2023-04-22 09:29:20|2023-04-22|  09|1680-2023-04-22-09| 1682156076|2023-04-22 09:34:36|\n",
      "|670697818|            68631|    1217|          I|       X| 101982239|       1|2023-04-22 05:16:29|                   68631|     true|       false|1682140589|2023-04-22 05:16:29|2023-04-22|  05|1217-2023-04-22-05|       null|               null|\n",
      "|670690191|            55899|    1102|          I|       X| 133470738|       1|2023-04-22 04:47:03|                   55899|     true|       false|1682138823|2023-04-22 04:47:03|2023-04-22|  04|1102-2023-04-22-04| 1682140978|2023-04-22 05:22:58|\n",
      "|670964719|           172479|     152|          I|       X| 146119761|       1|2023-04-22 12:59:12|                  172479|     true|       false|1682168352|2023-04-22 12:59:12|2023-04-22|  12| 152-2023-04-22-12|       null|               null|\n",
      "|670823901|            68641|    1229|          I|       X| 107656077|       1|2023-04-22 09:02:44|                   68641|     true|       false|1682154164|2023-04-22 09:02:44|2023-04-22|  09|1229-2023-04-22-09| 1682154241|2023-04-22 09:04:01|\n",
      "|670778303|           186011|     481|          I|       X|  78068679|       1|2023-04-22 07:56:34|                  186011|     true|       false|1682150194|2023-04-22 07:56:34|2023-04-22|  07| 481-2023-04-22-07| 1682150681|2023-04-22 08:04:41|\n",
      "|670723284|            55899|    1102|          I|       X|  68863373|       1|2023-04-22 06:20:01|                   55899|     true|       false|1682144401|2023-04-22 06:20:01|2023-04-22|  06|1102-2023-04-22-06| 1682144797|2023-04-22 06:26:37|\n",
      "|670912283|           186011|     481|          I|       X|  68761395|       1|2023-04-22 11:17:57|                  186011|     true|       false|1682162277|2023-04-22 11:17:57|2023-04-22|  11| 481-2023-04-22-11|       null|               null|\n",
      "|670703426|            55899|    1102|          I|       X| 133470738|       1|2023-04-22 05:32:14|                   55899|     true|       false|1682141534|2023-04-22 05:32:14|2023-04-22|  05|1102-2023-04-22-05| 1682143704|2023-04-22 06:08:24|\n",
      "|670963439|            68639|    1226|          I|       X| 130156577|       1|2023-04-22 12:56:23|                   68639|     true|       false|1682168183|2023-04-22 12:56:23|2023-04-22|  12|1226-2023-04-22-12| 1682168439|2023-04-22 13:00:39|\n",
      "|670953506|            68631|    1217|          I|       X|  75285775|       1|2023-04-22 12:34:19|                   68631|     true|       false|1682166859|2023-04-22 12:34:19|2023-04-22|  12|1217-2023-04-22-12|       null|               null|\n",
      "+---------+-----------------+--------+-----------+--------+----------+--------+-------------------+------------------------+---------+------------+----------+-------------------+----------+----+------------------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inbounds_matched.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d705b5d-0fec-464a-b8c2-8711c12eff27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
